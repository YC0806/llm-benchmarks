# LLM Benchmarks - Remote API Testing

åŸºäº vLLM åŸºå‡†æµ‹è¯•å·¥å…·æ”¹é€ çš„è½»é‡çº§è¿œç¨‹ API æµ‹è¯•æ¡†æ¶ï¼Œæ— éœ€å®‰è£… vLLMã€‚

## ğŸ¯ ç‰¹æ€§

- âœ… **é›¶ vLLM ä¾èµ–**ï¼šä¸“æ³¨äºè¿œç¨‹ API æµ‹è¯•ï¼Œä¸éœ€è¦æœ¬åœ°æ¨¡å‹éƒ¨ç½²
- âœ… **å¤šåç«¯æ”¯æŒ**ï¼šOpenAIã€vLLM Serverã€TGIã€TensorRT-LLM ç­‰
- âœ… **ä¸°å¯Œçš„æ•°æ®é›†**ï¼šShareGPTã€Randomã€HuggingFace ç­‰
- âœ… **å…¨é¢çš„æŒ‡æ ‡**ï¼šTTFTã€TPOTã€ITLã€E2ELã€ååé‡ç­‰
- âœ… **å¹¶å‘æµ‹è¯•**ï¼šæ”¯æŒå¤šå®¢æˆ·ç«¯å¹¶å‘ã€å¤šè½®å¯¹è¯æµ‹è¯•
- âœ… **çµæ´»çš„è¯·æ±‚é€Ÿç‡æ§åˆ¶**ï¼šå›ºå®šé€Ÿç‡ã€Poisson è¿‡ç¨‹ã€çº¿æ€§/æŒ‡æ•°é€’å¢

## ğŸ“¦ å®‰è£…

### æœ€å°åŒ–å®‰è£…ï¼ˆä»… 4 ä¸ªä¾èµ–ï¼‰â­
```bash
pip install numpy pandas aiohttp tqdm
```
> æ³¨æ„ï¼šToken ç»Ÿè®¡å°†ä½¿ç”¨ API å“åº”æ•°æ®æˆ–å­—ç¬¦ä¼°ç®—ï¼ˆè¯¯å·®çº¦ Â±10-20%ï¼‰

### æ¨èå®‰è£…ï¼ˆåŒ…å«ç²¾ç¡® token è®¡æ•°ï¼‰
```bash
pip install numpy pandas aiohttp tqdm transformers
```

### å¯é€‰ä¾èµ–

**Excel å¯¼å‡º**ï¼ˆå¤šè½®æµ‹è¯•ï¼‰ï¼š
```bash
pip install xlsxwriter
```
> æ³¨æ„ï¼šå¦‚æœä¸å®‰è£… xlsxwriterï¼Œä½¿ç”¨ `--excel-output` æ—¶ä¼šè‡ªåŠ¨é™çº§åˆ° CSV å¯¼å‡º

**å›¾åƒæ¨¡å‹æµ‹è¯•**ï¼š
```bash
pip install Pillow datasets
```

**éŸ³é¢‘æ¨¡å‹æµ‹è¯•**ï¼š
```bash
pip install soundfile librosa
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯• OpenAI API
```bash
export OPENAI_API_KEY="your-api-key"

python benchmark_serving.py \
    --backend openai-chat \
    --model gpt-4 \
    --base-url https://api.openai.com \
    --dataset-name random \
    --num-prompts 10 \
    --request-rate 1
```

### 2. æµ‹è¯• vLLM æœåŠ¡å™¨
```bash
# å‡è®¾ä½ çš„ vLLM æœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:8000
python benchmark_serving.py \
    --backend vllm \
    --model meta-llama/Llama-2-7b-hf \
    --base-url http://localhost:8000 \
    --dataset-name sharegpt \
    --dataset-path ./data/sharegpt.json \
    --num-prompts 100 \
    --request-rate 10
```

### 3. ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†
åˆ›å»º `custom_prompts.jsonl`ï¼š
```jsonl
{"prompt": "What is the capital of France?"}
{"prompt": "Explain quantum computing in simple terms."}
{"prompt": "Write a haiku about coding."}
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
python benchmark_serving.py \
    --backend openai-chat \
    --model gpt-4 \
    --base-url https://api.openai.com \
    --dataset-name custom \
    --dataset-path ./custom_prompts.jsonl \
    --num-prompts 3
```

### 4. å¤šè½®å¯¹è¯æµ‹è¯•
```bash
python multi_turn/benchmark_serving_multi_turn.py \
    -i ./data/conversations.json \
    -m gpt-4 \
    -u https://api.openai.com \
    -p 4 \
    --request-rate 2 \
    --excel-output
```

## ğŸ“Š æ”¯æŒçš„åç«¯

| åç«¯ | å‚æ•°å€¼ | è¯´æ˜ |
|------|--------|------|
| OpenAI Completions | `openai` | OpenAI æ ‡å‡†å®Œæˆ API |
| OpenAI Chat | `openai-chat` | OpenAI èŠå¤©å®Œæˆ API |
| OpenAI Audio | `openai-audio` | OpenAI è¯­éŸ³è¯†åˆ« API |
| vLLM Server | `vllm` | vLLM OpenAI å…¼å®¹æœåŠ¡å™¨ |
| TGI | `tgi` | HuggingFace Text Generation Inference |
| TensorRT-LLM | `tensorrt-llm` | NVIDIA TensorRT-LLM |
| LMDeploy | `lmdeploy` | LMDeploy æœåŠ¡å™¨ |
| SGLang | `sglang` | SGLang æœåŠ¡å™¨ |
| llama.cpp | `llama.cpp` | llama.cpp æœåŠ¡å™¨ |
| DeepSpeed-MII | `deepspeed-mii` | DeepSpeed MII æœåŠ¡å™¨ |
| ScaleLLM | `scalellm` | ScaleLLM æœåŠ¡å™¨ |

## ğŸ“ æ•°æ®é›†ç±»å‹

| æ•°æ®é›† | å‚æ•°å€¼ | è¯´æ˜ |
|--------|--------|------|
| Random | `random` | éšæœºç”Ÿæˆçš„åˆæˆæ•°æ® |
| ShareGPT | `sharegpt` | ShareGPT å¯¹è¯æ•°æ® |
| Sonnet | `sonnet` | è¯—æ­Œæ–‡æœ¬æ•°æ® |
| BurstGPT | `burstgpt` | çªå‘è¯·æ±‚æ¨¡å¼æ•°æ® |
| Custom | `custom` | è‡ªå®šä¹‰ JSONL æ ¼å¼ |
| HuggingFace | `hf` | HuggingFace æ•°æ®é›† |

## ğŸ“ˆ åŸºå‡†æµ‹è¯•æŒ‡æ ‡

- **TTFT** (Time to First Token): é¦–ä¸ª token ç”Ÿæˆæ—¶é—´
- **TPOT** (Time per Output Token): æ¯ä¸ªè¾“å‡º token å¹³å‡æ—¶é—´
- **ITL** (Inter-token Latency): token é—´å»¶è¿Ÿ
- **E2EL** (End-to-end Latency): ç«¯åˆ°ç«¯å»¶è¿Ÿ
- **Throughput**: ååé‡ï¼ˆtokens/s æˆ– requests/sï¼‰
- **Goodput**: æ»¡è¶³ SLO çš„è¯·æ±‚ååé‡

## ğŸ”§ å¸¸ç”¨å‚æ•°

### åŸºç¡€å‚æ•°
```bash
--backend <backend>           # åç«¯ç±»å‹
--model <model_name>          # æ¨¡å‹åç§°
--base-url <url>              # API åŸºç¡€ URL
--dataset-name <name>         # æ•°æ®é›†ç±»å‹
--dataset-path <path>         # æ•°æ®é›†è·¯å¾„ï¼ˆå¦‚éœ€è¦ï¼‰
```

### æ€§èƒ½æ§åˆ¶
```bash
--num-prompts <n>             # è¯·æ±‚æ•°é‡
--request-rate <rate>         # è¯·æ±‚é€Ÿç‡ï¼ˆRPSï¼‰ï¼Œinf è¡¨ç¤ºæ— é™åˆ¶
--max-concurrency <n>         # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
--burstiness <factor>         # çªå‘å› å­ï¼ˆ1.0 = Poissonï¼‰
```

### è¯·æ±‚é€Ÿç‡é€’å¢
```bash
--ramp-up-strategy <strategy> # linear æˆ– exponential
--ramp-up-start-rps <n>       # èµ·å§‹ RPS
--ramp-up-end-rps <n>         # ç»“æŸ RPS
```

### è¾“å‡ºæ§åˆ¶
```bash
--save-result                 # ä¿å­˜ç»“æœåˆ° JSON
--result-dir <dir>            # ç»“æœä¿å­˜ç›®å½•
--result-filename <name>      # ç»“æœæ–‡ä»¶å
--disable-tqdm                # ç¦ç”¨è¿›åº¦æ¡
```

### Tokenizer é…ç½®
```bash
--tokenizer <path>            # Tokenizer è·¯å¾„ï¼ˆå¯é€‰ï¼‰
--tokenizer-mode <mode>       # autoï¼ˆé»˜è®¤ï¼‰æˆ– slow
--trust-remote-code           # ä¿¡ä»»è¿œç¨‹ä»£ç ï¼ˆTokenizerï¼‰
```

## ğŸ’¡ é«˜çº§ç”¨æ³•

### 1. æ€§èƒ½å‹æµ‹ï¼ˆæœ€å¤§ååé‡ï¼‰
```bash
python benchmark_serving.py \
    --backend vllm \
    --model meta-llama/Llama-2-7b-hf \
    --base-url http://localhost:8000 \
    --dataset-name random \
    --num-prompts 1000 \
    --request-rate inf \
    --max-concurrency 100
```

### 2. ç¨³å®šæ€§æµ‹è¯•ï¼ˆå›ºå®šé€Ÿç‡ï¼‰
```bash
python benchmark_serving.py \
    --backend openai-chat \
    --model gpt-4 \
    --base-url https://api.openai.com \
    --dataset-name sharegpt \
    --dataset-path ./data/sharegpt.json \
    --num-prompts 500 \
    --request-rate 10 \
    --burstiness 1.0
```

### 3. æ¸è¿›å¼å‹æµ‹ï¼ˆæµé‡é€’å¢ï¼‰
```bash
python benchmark_serving.py \
    --backend vllm \
    --model meta-llama/Llama-2-7b-hf \
    --base-url http://localhost:8000 \
    --dataset-name random \
    --num-prompts 1000 \
    --ramp-up-strategy linear \
    --ramp-up-start-rps 1 \
    --ramp-up-end-rps 50
```

### 4. Goodput æµ‹è¯•ï¼ˆSLO çº¦æŸï¼‰
```bash
python benchmark_serving.py \
    --backend vllm \
    --model meta-llama/Llama-2-7b-hf \
    --base-url http://localhost:8000 \
    --dataset-name sharegpt \
    --dataset-path ./data/sharegpt.json \
    --num-prompts 200 \
    --request-rate 10 \
    --goodput ttft:200 tpot:50 e2el:5000
```

### 5. å¤šå®¢æˆ·ç«¯å¹¶å‘æµ‹è¯•
```bash
python multi_turn/benchmark_serving_multi_turn.py \
    -i ./data/conversations.json \
    -m gpt-4 \
    -u https://api.openai.com \
    -p 10 \
    -k 50 \
    --request-rate 5 \
    --max-num-requests 1000
```

## ğŸ“ æ•°æ®é›†æ ¼å¼

### ShareGPT æ ¼å¼
```json
[
  {
    "conversations": [
      {"role": "user", "value": "Hello!"},
      {"role": "assistant", "value": "Hi! How can I help?"}
    ]
  }
]
```

### Custom æ ¼å¼ï¼ˆJSONLï¼‰
```jsonl
{"prompt": "Question 1"}
{"prompt": "Question 2"}
{"prompt": "Question 3"}
```

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ï¼šæ‰¾ä¸åˆ° vLLM æ¨¡å—
è¿™æ˜¯æ­£å¸¸çš„ï¼æœ¬é¡¹ç›®å·²ç§»é™¤ vLLM ä¾èµ–ï¼Œä¸éœ€è¦å®‰è£… vLLMã€‚

### é—®é¢˜ï¼šMistral tokenizer é”™è¯¯
**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ `--tokenizer-mode auto`ï¼ˆé»˜è®¤ï¼‰æˆ– `--tokenizer-mode slow`

### é—®é¢˜ï¼šè¿æ¥è¶…æ—¶
**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥ `--base-url` æ˜¯å¦æ­£ç¡®
2. ç¡®ä¿ API æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ
3. æ£€æŸ¥é˜²ç«å¢™è®¾ç½®

### é—®é¢˜ï¼šToken è®¡æ•°ä¸å‡†ç¡®
**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨ `--tokenizer` å‚æ•°æŒ‡å®šä¸æœåŠ¡å™¨ç›¸åŒçš„ tokenizer

## ğŸ”„ ä» vLLM ç‰ˆæœ¬è¿ç§»

è¯¦è§ [VLLM_DEPENDENCY_REMOVAL.md](VLLM_DEPENDENCY_REMOVAL.md)

ä¸»è¦å˜åŒ–ï¼š
- âœ… ä¸å†éœ€è¦å®‰è£… vLLM
- âœ… æ‰€æœ‰è¿œç¨‹ API åŠŸèƒ½ä¿ç•™
- âŒ ä¸æ”¯æŒæœ¬åœ°æ¨¡å‹éƒ¨ç½²
- âŒ ä¸æ”¯æŒ Mistral tokenizer æ¨¡å¼
- âŒ ä¸æ”¯æŒæœ¬åœ° LoRA é€‚é…å™¨

## ğŸ“š å‚è€ƒèµ„æ–™

- [åŸå§‹ READMEï¼ˆvLLM ç‰ˆæœ¬ï¼‰](README_original.md)
- [vLLM é¡¹ç›®](https://github.com/vllm-project/vllm)
- [OpenAI API æ–‡æ¡£](https://platform.openai.com/docs/api-reference)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

Apache-2.0 License

---

**é¡¹ç›®çŠ¶æ€**: æ´»è·ƒå¼€å‘ä¸­
**æœ€åæ›´æ–°**: 2025-10-28
**ç‰ˆæœ¬**: 2.0 (vLLM-free)
