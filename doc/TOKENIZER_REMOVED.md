# Transformers/Tokenizer ä¾èµ–å®Œå…¨ç§»é™¤è¯´æ˜

## ğŸ“‹ æ¦‚è¿°

**transformers** åº“å’Œ **tokenizer** å·²**å®Œå…¨ç§»é™¤**ã€‚æœ¬é¡¹ç›®ç°åœ¨ä¸“ä¸º**ç¦»çº¿/å†…ç½‘ç¯å¢ƒ**è®¾è®¡ï¼Œä¸ä¾èµ–ä»»ä½•éœ€è¦å¤–éƒ¨ç½‘ç»œä¸‹è½½çš„åº“ã€‚

## âœ… å®Œæˆçš„ä¿®æ”¹

### 1. ä»£ç ä¿®æ”¹

**ä¿®æ”¹çš„æ–‡ä»¶**ï¼š
- âœ… [backend_request_func.py](backend_request_func.py) - å®Œå…¨ç§»é™¤ transformers å¯¼å…¥ï¼Œåˆ é™¤ `get_tokenizer()` å‡½æ•°
- âœ… [benchmark_serving.py](benchmark_serving.py) - ç§»é™¤æ‰€æœ‰ tokenizer å‚æ•°å’Œå¼•ç”¨
- âœ… [benchmark_dataset.py](benchmark_dataset.py) - å°† `PreTrainedTokenizerBase` æ›¿æ¢ä¸º `Any`
- âœ… [requirements.txt](requirements.txt) - ç§»é™¤ transformers å’Œå…¶ä»–éœ€è¦ç½‘ç»œä¸‹è½½çš„ä¾èµ–

**æ ¸å¿ƒå˜åŒ–**ï¼š
```python
# ä¹‹å‰ï¼ˆå¿…éœ€ï¼‰
from transformers import PreTrainedTokenizerBase
tokenizer = get_tokenizer(model_name)

# ç°åœ¨ï¼ˆå®Œå…¨ç§»é™¤ï¼‰
# NO transformers import
# NO tokenizer
# Token counting uses API responses or character-based estimation
```

### 2. Token è®¡æ•°ç­–ç•¥

**æ–°çš„å•ä¸€ç­–ç•¥**ï¼š

```python
Priority 1: API å“åº”ä¸­çš„ token ç»Ÿè®¡ï¼ˆæœ€å‡†ç¡®ï¼‰
    â†“
Priority 2: å­—ç¬¦æ•°ä¼°ç®—ï¼ˆè¿‘ä¼¼ï¼ŒÂ±10-20% è¯¯å·®ï¼‰
```

**ä¼°ç®—å…¬å¼**ï¼š
```python
def estimate_token_count(text: str) -> int:
    """
    Estimate token count based on character count.

    - ä¸­æ–‡/CJK: ~1.5 ä¸ªå­—ç¬¦ = 1 token
    - è‹±æ–‡/ASCII: ~4 ä¸ªå­—ç¬¦ = 1 token
    """
    cjk_tokens = cjk_count / 1.5
    non_cjk_tokens = non_cjk_count / 4.0
    return int(cjk_tokens + non_cjk_tokens)
```

### 3. ç§»é™¤çš„æ•°æ®é›†

ä»¥ä¸‹éœ€è¦å¤–éƒ¨ä¸‹è½½æˆ– tokenizer çš„æ•°æ®é›†å·²**å®Œå…¨ç¦ç”¨**ï¼š

- âŒ HuggingFace æ•°æ®é›† (hf) - éœ€è¦ä»äº’è”ç½‘ä¸‹è½½
- âŒ ShareGPT - éœ€è¦ä»äº’è”ç½‘ä¸‹è½½
- âŒ BurstGPT - éœ€è¦ä»äº’è”ç½‘ä¸‹è½½
- âŒ Random - éœ€è¦ tokenizer
- âŒ Sonnet (é openai-chat åç«¯) - éœ€è¦ tokenizer å’Œ chat_template

**ä»…æ”¯æŒçš„æ•°æ®é›†**ï¼š
- âœ… **custom** - ä½¿ç”¨æœ¬åœ° JSONL æ–‡ä»¶ï¼ˆæ¨èï¼‰
- âœ… **sonnet** - ä»… openai-chat åç«¯

---

## ğŸš€ ä½¿ç”¨æ–¹å¼

### æœ€å°åŒ–å®‰è£…ï¼ˆ4ä¸ªæ ¸å¿ƒä¾èµ–ï¼‰

```bash
# åªå®‰è£…æ ¸å¿ƒä¾èµ–
pip install numpy pandas aiohttp tqdm

# è¿è¡Œæµ‹è¯•
python benchmark_serving.py \
    --backend openai-chat \
    --model gpt-4 \
    --base-url https://api.openai.com \
    --dataset-name custom \
    --dataset-path online_query_etl_100.jsonl \
    --num-prompts 10
```

**æ— è­¦å‘Šè¾“å‡º** - tokenizer å·²å®Œå…¨ç§»é™¤ï¼Œä¸ä¼šæ˜¾ç¤ºä»»ä½• tokenizer ç›¸å…³è­¦å‘Šã€‚

### æ¨èç”¨æ³•ï¼šä½¿ç”¨æœ¬åœ° JSONL æ–‡ä»¶

```bash
# ä½¿ç”¨ä¾¿æ·è„šæœ¬
python test_with_jsonl.py \
    --model gpt-4 \
    --num-prompts 50 \
    --request-rate 5 \
    --save-result
```

è¯¦è§ï¼š[JSONL_TESTING_GUIDE.md](JSONL_TESTING_GUIDE.md)

---

## ğŸ“Š åŠŸèƒ½æ”¯æŒ

| åŠŸèƒ½ | æ”¯æŒçŠ¶æ€ | è¯´æ˜ |
|------|---------|------|
| **Custom æ•°æ®é›†** | âœ… å®Œå…¨æ”¯æŒ | ä½¿ç”¨æœ¬åœ° JSONL æ–‡ä»¶ |
| **Sonnet æ•°æ®é›†** | âš ï¸ éƒ¨åˆ†æ”¯æŒ | ä»… openai-chat åç«¯ |
| **Token è®¡æ•°** | âœ… æ”¯æŒ | API å“åº” + å­—ç¬¦ä¼°ç®— |
| **Input token ç»Ÿè®¡** | âš ï¸ ä¼°ç®— | åŸºäºå­—ç¬¦æ•°ï¼ŒÂ±10-20% è¯¯å·® |
| **Output token ç»Ÿè®¡** | âœ… ç²¾ç¡® | å¤§å¤šæ•° API è¿”å›ç²¾ç¡®å€¼ |
| **ç¦»çº¿ç¯å¢ƒ** | âœ… å®Œå…¨æ”¯æŒ | æ— éœ€å¤–éƒ¨ç½‘ç»œ |
| **HF æ•°æ®é›†** | âŒ ä¸æ”¯æŒ | éœ€è¦å¤–éƒ¨ä¸‹è½½ |
| **Chat template** | âŒ ä¸æ”¯æŒ | å·²ç§»é™¤ |

---

## âš ï¸ ä½¿ç”¨é™åˆ¶ä¸è¯´æ˜

### 1. Token ç»Ÿè®¡ç²¾åº¦

**å½“å‰æ–¹æ¡ˆ**ï¼š
- **Input tokens**: ä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®—ï¼Œè¯¯å·®çº¦ Â±10-20%
- **Output tokens**:
  - âœ… å¦‚æœ API è¿”å› token æ•°ï¼ˆå¦‚ OpenAIã€vLLMï¼‰ï¼Œä½¿ç”¨ API æ•°æ®ï¼ˆ**ç²¾ç¡®**ï¼‰
  - âš ï¸ å¦‚æœ API ä¸è¿”å› token æ•°ï¼Œä½¿ç”¨å­—ç¬¦ä¼°ç®—ï¼ˆ**è¿‘ä¼¼**ï¼‰

**ç¤ºä¾‹å¯¹æ¯”**ï¼š
```
æ–‡æœ¬: "ç¬¬äºŒä»£è®¡ç®—æœºé‡‡ç”¨ä»€ä¹ˆä½œä¸ºé€»è¾‘å…ƒä»¶"
- å­—ç¬¦ä¼°ç®—: ~18 tokens
- å®é™… tokens: ~15 tokens
- è¯¯å·®: +20%

æ–‡æœ¬: "What is artificial intelligence?"
- å­—ç¬¦ä¼°ç®—: ~8 tokens
- å®é™… tokens: ~5 tokens
- è¯¯å·®: +60%
```

**é‡è¦è¯´æ˜**ï¼š
- âœ… å¯¹äº**ç›¸å¯¹æ€§èƒ½æ¯”è¾ƒ**ï¼Œå­—ç¬¦ä¼°ç®—ä»ç„¶æœ‰æ•ˆ
- âœ… å¤§å¤šæ•°ç°ä»£ LLM API éƒ½ä¼šè¿”å›ç²¾ç¡®çš„ token æ•°
- âš ï¸ Input token ç»Ÿè®¡æ˜¯ä¼°ç®—å€¼ï¼Œä½†ä¸å½±å“å®é™… API è°ƒç”¨
- âš ï¸ ç”¨æˆ·æ˜ç¡®çŸ¥æ‚‰æ­¤æ–¹æ¡ˆçš„é£é™©

### 2. ä¸æ”¯æŒçš„æ•°æ®é›†

å°è¯•ä½¿ç”¨ä»¥ä¸‹æ•°æ®é›†ä¼šæŠ¥é”™ï¼š
```python
ValueError: Unsupported dataset: sharegpt.
In offline mode, only 'custom' and 'sonnet' datasets are supported.
Use 'custom' dataset with your own JSONL file for testing.
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨ `--dataset-name custom` å’Œ `--dataset-path your_data.jsonl`
2. å‚è€ƒ [online_query_etl_100.jsonl](online_query_etl_100.jsonl) çš„æ ¼å¼åˆ›å»ºè‡ªå·±çš„æ•°æ®

### 3. Sonnet æ•°æ®é›†é™åˆ¶

å¯¹äº Sonnet æ•°æ®é›†ï¼š
- âœ… `openai-chat` åç«¯å¯ç”¨
- âŒ å…¶ä»–åç«¯ä¼šæŠ¥é”™ï¼š
  ```
  ValueError: Sonnet dataset only supports 'openai-chat' backend in offline mode.
  For other backends, use 'custom' dataset with your own JSONL file.
  ```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### åœºæ™¯ 1: å†…ç½‘/ç¦»çº¿ç¯å¢ƒæµ‹è¯•ï¼ˆæ¨èï¼‰
```bash
# æœ€å°åŒ–å®‰è£…ï¼Œé€‚åˆå†…ç½‘ç¯å¢ƒ
pip install numpy pandas aiohttp tqdm

# ä½¿ç”¨è‡ªå·±çš„ JSONL æ•°æ®
python test_with_jsonl.py \
    --model your-model \
    --base-url http://your-internal-api \
    --backend openai-chat \
    --num-prompts 100 \
    --save-result
```
**ä¼˜åŠ¿**ï¼š
- âœ… æ— éœ€å¤–éƒ¨ç½‘ç»œ
- âœ… å¿«é€Ÿå®‰è£…ï¼ˆ4ä¸ªä¾èµ–ï¼‰
- âœ… ä½å†…å­˜å ç”¨
- âœ… API è¿”å› token æ•°å·²è¶³å¤Ÿç²¾ç¡®

### åœºæ™¯ 2: å…¬æœ‰äº‘ API æµ‹è¯•
```bash
# OpenAI/Azure/å…¶ä»–å…¬æœ‰äº‘
python test_with_jsonl.py \
    --model gpt-4 \
    --base-url https://api.openai.com \
    --num-prompts 50 \
    --request-rate 5
```
**ä¼˜åŠ¿**ï¼š
- âœ… API è¿”å›ç²¾ç¡® token ç»Ÿè®¡
- âœ… æ— éœ€ tokenizer

### åœºæ™¯ 3: ç§æœ‰éƒ¨ç½² LLM æµ‹è¯•
```bash
# vLLM/LMDeploy/SGLang/TGI
python test_with_jsonl.py \
    --model Qwen/Qwen2.5-7B-Instruct \
    --base-url http://localhost:8000 \
    --backend vllm \
    --num-prompts 100
```

---

## ğŸ” API Token ç»Ÿè®¡æ”¯æŒ

ä¸åŒ API å¯¹ token ç»Ÿè®¡çš„æ”¯æŒï¼š

| API æä¾›å•† | è¿”å› Input Tokens | è¿”å› Output Tokens | ä¼°ç®—è¯¯å·® |
|-----------|-------------------|-------------------|---------|
| **OpenAI** | âœ… Yes | âœ… Yes | N/A (ç²¾ç¡®) |
| **vLLM Server** | âœ… Yes | âœ… Yes | N/A (ç²¾ç¡®) |
| **LMDeploy** | âœ… Yes | âœ… Yes | N/A (ç²¾ç¡®) |
| **SGLang** | âœ… Yes | âœ… Yes | N/A (ç²¾ç¡®) |
| **TGI** | âš ï¸ Partial | âœ… Yes | Â±10-20% |
| **TensorRT-LLM** | âš ï¸ Depends | âš ï¸ Depends | Â±10-20% |
| **è‡ªå®šä¹‰ API** | âš ï¸ Depends | âš ï¸ Depends | Â±10-20% |

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: æ‰¾ä¸åˆ° transformers

**ç—‡çŠ¶**ï¼š
```
ModuleNotFoundError: No module named 'transformers'
```

**è¯´æ˜**ï¼šè¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸º transformers å·²å®Œå…¨ç§»é™¤ã€‚å¦‚æœçœ‹åˆ°æ­¤é”™è¯¯ï¼š
1. ç¡®ä¿ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ä»£ç 
2. æ¸…ç† Python ç¼“å­˜ï¼š`find . -type d -name __pycache__ -exec rm -rf {} +`

### é—®é¢˜ 2: æ•°æ®é›†ä¸æ”¯æŒ

**ç—‡çŠ¶**ï¼š
```
ValueError: Unsupported dataset: sharegpt
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ”¹ç”¨ custom æ•°æ®é›†
python benchmark_serving.py \
    --dataset-name custom \
    --dataset-path your_data.jsonl \
    ...
```

### é—®é¢˜ 3: Token ç»Ÿè®¡ä¸å‡†ç¡®

**ç—‡çŠ¶**ï¼š
```
Input token throughput (tok/s): 250.5
ï¼ˆä¸é¢„æœŸä¸ç¬¦ï¼‰
```

**è¯´æ˜**ï¼š
- Input token ä½¿ç”¨å­—ç¬¦ä¼°ç®—ï¼Œæœ‰ Â±10-20% è¯¯å·®
- è¿™æ˜¯**é¢„æœŸè¡Œä¸º**ï¼Œç”¨æˆ·å·²çŸ¥æ‚‰æ­¤é£é™©
- å¯¹äºç›¸å¯¹æ€§èƒ½æ¯”è¾ƒä»ç„¶æœ‰æ•ˆ
- Output token å¤§å¤šæ•°æƒ…å†µä¸‹æ˜¯ç²¾ç¡®çš„ï¼ˆæ¥è‡ª APIï¼‰

---

## ğŸ“ˆ è¯¯å·®åˆ†æ

### å­—ç¬¦ä¼°ç®— vs å®é™… Tokenï¼ˆå‚è€ƒæ•°æ®ï¼‰

æµ‹è¯•æ•°æ®ï¼ˆ100 æ¡ä¸­æ–‡å¯¹è¯ï¼‰ï¼š

| ç»Ÿè®¡é¡¹ | å­—ç¬¦ä¼°ç®— | å®é™… Tokens | è¯¯å·® |
|--------|---------|-------------|------|
| å¹³å‡ Input | 56 | 48 | +16.7% |
| å¹³å‡ Output | 182 | 165* | +10.3%* |
| æ€»ä½“ | 238 | 213 | +11.7% |

*æ³¨ï¼šOutput token é€šå¸¸ç”± API è¿”å›ï¼Œæ­¤å¤„ä¸ºå¯¹æ¯”æµ‹è¯•æ•°æ®

**ç»“è®º**ï¼š
- ä¸­æ–‡æ–‡æœ¬ä¼°ç®—è¯¯å·®çº¦ 10-15%
- è‹±æ–‡æ–‡æœ¬ä¼°ç®—è¯¯å·®çº¦ 20-30%
- å¯¹äº**ç›¸å¯¹æ€§èƒ½æ¯”è¾ƒ**ä»ç„¶æœ‰æ•ˆ
- ä¸å½±å“å®é™… API è°ƒç”¨å’Œååé‡æµ‹è¯•

---

## ğŸ¯ ä¸ºä»€ä¹ˆå®Œå…¨ç§»é™¤ï¼Ÿ

### è®¾è®¡ç›®æ ‡

1. **ç¦»çº¿/å†…ç½‘ç¯å¢ƒæ”¯æŒ**
   - æµ‹è¯•åœºæ™¯é€šå¸¸åœ¨æ— æ³•è¿æ¥å¤–éƒ¨ç½‘ç»œçš„ç¯å¢ƒä¸­
   - transformers éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶å’Œè¯è¡¨
   - huggingface_hub éœ€è¦è®¿é—®å¤–éƒ¨ API

2. **æœ€å°åŒ–ä¾èµ–**
   - ä» 10+ ä¸ªä¾èµ–å‡å°‘åˆ° **4 ä¸ªæ ¸å¿ƒä¾èµ–**
   - æ›´å¿«çš„å®‰è£…å’Œéƒ¨ç½²
   - æ›´ä½çš„å†…å­˜å ç”¨

3. **API æµ‹è¯•åœºæ™¯**
   - é¡¹ç›®åªæµ‹è¯•è¿œç«¯å¤§æ¨¡å‹ API
   - ä¸æ¶‰åŠæœ¬åœ°æ¨¡å‹éƒ¨ç½²
   - API é€šå¸¸è¿”å›ç²¾ç¡®çš„ token ç»Ÿè®¡

### æƒè¡¡å–èˆ

| æ–¹é¢ | ç§»é™¤å‰ | ç§»é™¤å |
|------|--------|--------|
| **æ ¸å¿ƒä¾èµ–æ•°é‡** | 10+ | 4 |
| **å®‰è£…å¤§å°** | ~2GB | ~50MB |
| **å¯åŠ¨é€Ÿåº¦** | ~5s | ~1s |
| **å†…å­˜å ç”¨** | ~1GB | ~100MB |
| **Input token ç²¾åº¦** | Â±1% | Â±10-20% |
| **Output token ç²¾åº¦** | Â±1% | ç²¾ç¡®ï¼ˆAPIè¿”å›ï¼‰ |
| **ç¦»çº¿ç¯å¢ƒæ”¯æŒ** | âŒ | âœ… |

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [VLLM_DEPENDENCY_REMOVAL.md](VLLM_DEPENDENCY_REMOVAL.md) - vLLM ä¾èµ–ç§»é™¤è¯´æ˜
- [XLSXWRITER_OPTIONAL.md](XLSXWRITER_OPTIONAL.md) - xlsxwriter å¯é€‰ä¾èµ–
- [JSONL_TESTING_GUIDE.md](JSONL_TESTING_GUIDE.md) - JSONL æ•°æ®æµ‹è¯•æŒ‡å—
- [requirements.txt](requirements.txt) - ä¾èµ–åˆ—è¡¨ï¼ˆä»…4ä¸ªæ ¸å¿ƒä¾èµ–ï¼‰

---

## ğŸ”„ ä»æ—§ç‰ˆæœ¬è¿ç§»

### å¦‚æœä¹‹å‰å®‰è£…äº† transformers

```bash
# 1. å¸è½½ä¸éœ€è¦çš„ä¾èµ–
pip uninstall transformers huggingface-hub tokenizers -y

# 2. æ¸…ç†ç¼“å­˜
rm -rf ~/.cache/huggingface

# 3. é‡æ–°å®‰è£…æ ¸å¿ƒä¾èµ–
pip install numpy pandas aiohttp tqdm

# 4. è¿è¡Œæµ‹è¯•
python test_with_jsonl.py --model gpt-4 --num-prompts 5
```

### è¿ç§»æ•°æ®é›†

**ä¹‹å‰ä½¿ç”¨ ShareGPT/HuggingFace æ•°æ®é›†**ï¼š
```bash
# æ”¹ç”¨ custom æ•°æ®é›†
python benchmark_serving.py \
    --dataset-name custom \
    --dataset-path your_local_data.jsonl \
    ...
```

**JSONL æ ¼å¼ç¤ºä¾‹**ï¼š
```json
[
  {"role": "user", "content": "ä½ å¥½"},
  {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
]
```

---

**æ€»ç»“**ï¼š

1. âœ… Tokenizer å’Œ transformers **å·²å®Œå…¨ç§»é™¤**
2. âœ… ä¸“ä¸º**ç¦»çº¿/å†…ç½‘ç¯å¢ƒ**è®¾è®¡
3. âœ… ä»…éœ€ **4 ä¸ªæ ¸å¿ƒä¾èµ–**
4. âš ï¸ Input token ä½¿ç”¨å­—ç¬¦ä¼°ç®—ï¼ˆÂ±10-20% è¯¯å·®ï¼‰
5. âœ… Output token å¤§å¤šæ•°æƒ…å†µç²¾ç¡®ï¼ˆAPI è¿”å›ï¼‰
6. âœ… ç”¨æˆ·**æ˜ç¡®çŸ¥æ‚‰**æ­¤æ–¹æ¡ˆçš„æƒè¡¡
